lda_model<-fit
}
}
Xstar <- rbind(identity, sqrt(select_lam)*t(S))
for(i in 1:n_test){
x_obs <- c(Xtest[i,], rep(0, p))
smooth.X_test[i,] <- as.vector(glmnet(Xstar, x_obs, lambda = 0, alpha=0,intercept = FALSE, standardize = FALSE, thresh = 1e-7)$beta)
}
df.test <- data.frame(cbind(smooth.X_test, y=ytest))
fitted.y <- predict(lda_model, df.test)$class
accuracy <- mean(ytest == fitted.y)
return(list(yhat = fitted.y, accuracy = accuracy))
}
library(R.matlab)
library(Matrix)
source('../opt.score_new1.R')
source('../cv_opt.score_new1.R')
source('../test and results.R')
eigV <- readMat('eigV.mat')$eigV
R0 <- readMM('../R0_642.mtx')
R1 <- readMM('../R1_642.mtx')
## sample generator
mysamples <- function(eigV, K=3, n=100){
r <- nrow(eigV)
c <- ncol(eigV)
X <- matrix(0, nrow=(K*n), ncol=r) # n samples by r nodes
for(k in 1:K){
for(i in 1:n){
# sample different bases v_1, v_2, v_3 for each individual
base <- eigV[, 1:3]
# sample 3 by 1 mean 0 coefficients of bases
coef <- matrix(c(rnorm(1,0,5), rnorm(1,0,3), rnorm(1,0,1)), nrow=3)
# add different mean vector to each class
mean_k <- 0.3*eigV[,(3+k)]
# the distribution of coefficients are same in the same class
x <- base%*%coef + rnorm(642, 0, 0.1) + mean_k
X[(k-1)*n+i,] <- x
}
}
y <- Reduce(rbind, sapply(1:K, function(x) rep(x,n)))
data <- cbind(y=y, X)
new_data <- data[sample(nrow(data)),]
y <- new_data[,1]
X <- new_data[,2:ncol(new_data)]
y_indice<-Reduce(rbind,lapply(y,function(x) {
y_i <- rep(0,K)
y_i[x]=1
return(y_i)
}))
df <- data.frame(X=X, y_indice=y_indice, y=y)
return(df)
}
## pre.smooth
pre.smooth <- function(ytrain, ytest, Xtrain, Xtest, R0, R1, lambdas){
n_train <- nrow(Xtrain)
n_test <- nrow(Xtest)
p <- ncol(Xtrain)
smooth.X_train <- matrix(0, nrow=n_train, ncol=p)
smooth.X_test <- matrix(0, nrow=n_test, ncol=p)
identity <- Diagonal(x=rep(1,p))
R0til.inv <- Matrix::Diagonal(x=1/Matrix::rowSums(R0))
S <- R1 %*% sqrt(R0til.inv)
max <- 0
select_lam <- 0
lda_model <- NULL
for(j in 1:length(lambdas)){
lambda <- lambdas[j]
Xstar <- rbind(identity, sqrt(lambda)*t(S))
for(i in 1:n_train){
x_obs <- c(Xtrain[i,], rep(0, p))
smooth.X_train[i,] <- as.vector(glmnet(Xstar, x_obs, lambda = 0, alpha=0,intercept = FALSE, standardize = FALSE, thresh = 1e-7)$beta)
}
df.train <- data.frame(cbind(smooth.X_train, y=ytrain))
fit <- lda(y~., df.train)
fitted.y <- predict(fit, df.train)$class
accuracy <- mean(ytrain == fitted.y)
if(accuracy>max) {
max <- accuracy
select_lam <- lambda
lda_model <- fit
}
}
Xstar <- rbind(identity, sqrt(select_lam)*t(S))
for(i in 1:n_test){
x_obs <- c(Xtest[i,], rep(0, p))
smooth.X_test[i,] <- as.vector(glmnet(Xstar, x_obs, lambda = 0, alpha=0,intercept = FALSE, standardize = FALSE, thresh = 1e-7)$beta)
}
df.test <- data.frame(cbind(smooth.X_test, y=ytest))
fitted.y <- predict(lda_model, df.test)$class
accuracy <- mean(ytest == fitted.y)
return(list(yhat = fitted.y, accuracy = accuracy))
}
samples <- mysamples(eigV,3,100)
samples <- samples[order(samples$y),]
test.index <- c(sample(1:n1, n1/3),sample(1:n2, n2/3),sample(1:n3, n3/3))
X <- as.matrix(samples[,1:642])
Y <- as.matrix(samples[,c("y_indice.1","y_indice.2","y_indice.3")])
y <- as.numeric(samples[,"y"])
X.train <- X[-test.index,]
Y.train <- Y[-test.index,]
y.train <- y[-test.index]
X.test <- X[test.index,]
Y.test <- Y[test.index,]
y.test <- y[test.index]
my.lambda.seq <- 10^seq(-5,5,1)
result2 <- rep(0, 100)
for(i in 1:100){
result.presmooth <- pre.smooth(y.train, y.test, X.train, X.test, R0, R1, my.lambda.seq)
result2[i] <- result.presmooth$accuracy
}
# for(i in 1:100){
#         fit.cv.presmooth <- cv_pre.smooth(y.train, X.train, R0, R1, 5, my.lambda.seq)
#         result.presmooth <- pre.smooth(y.train, X.train, R0, R1, fit.cv.presmooth$min_lambda)
#         result2[i] <- result.presmooth$accuracy
# }
boxplot(result2)
# Arguments:
# Y:          n by K indicator matrix of classes (response)
# X:          n by s design matrix (covariates)
# R0:         s by s symmetric mass matrix
# R1:         s by s symmetric stiffness matrix
# kfolds:     the number of folds
# lambdas:    a set of tuning parameters of smoothness penalty
# ------------------------------------------------------------------------
# Outputs:
# beta:       s by K-1 matrix beta
# min_lambda: the tuning parameter which results in minimum error
# cv_mse:     the error rate
cv_opt.score <- function(Y, X, R0, R1, kfolds, lambdas){
K <- ncol(Y)
nlambda <- length(lambdas)
cv_mse <- rep(0, nlambda)
fold <- sample(1:5, nrow(X), replace = TRUE)
R0til.inv <- Matrix::Diagonal(x=1/Matrix::rowSums(R0))
S<-R1 %*% sqrt(R0til.inv)
for(i in 1:nlambda){
print(i)
error=0
for(j in 1:kfolds){
X_train <- X[!fold==j,]
Y_train <- Y[!fold==j,]
X_valid <- X[fold==j,]
Y_valid <- Y[fold==j,]
model <- opt.score(Y_train, X_train, R0, R1, lambdas[i])
beta<-model$beta
mean_fac<-model$mean_fac
D <- 1/nrow(Y_valid) * (t(Y_valid)%*%Y_valid) # K by K matrix
#L <- R1 %*% R0til.inv %*% R1
#S <- Matrix::chol(L)
#centered.X<-scale(X_valid,center=TRUE,scale=FALSE)
centered.X<-sapply(1:length(mean_fac),function(x) X_valid[,x]-mean_fac[x])
scoredX <- matrix(rep(0, nrow(Y_valid)*(K-1)), ncol=K-1)
for(k in 1:(K-1)){
scoredX[,k] <- as.vector(centered.X %*% beta[,k])
}
## LDA with two discriminant directions
mu_k <- matrix(0, nrow = K-1, ncol = K) # centroid vectors
pi_k <- vector(length = K) # priori probabilities
sigma <- matrix(0, nrow = K-1, ncol = K-1) # covariance matrix
for (l in 1:K){
tmp <- matrix(scoredX[Y_valid[,l]==1,],ncol=K-1)
pi_k[l] <- nrow(tmp)/nrow(Y_valid)
mu_k[,l] <- colMeans(tmp)
for(c in 1:nrow(tmp)){
sigma <- sigma + (tmp[c,] - mu_k[,l]) %*% t(tmp[c,] - mu_k[,l])
}
}
sigma <- sigma*(1/(nrow(Y_valid)-K))
inv.sigma <- solve(sigma)
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y_valid))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhattrain <- apply(deltatrain, 1, which.max)
Y_valid_label <- apply(Y_valid, 1, which.max)
error <- error + mean(yhattrain!=Y_valid_label) # training error rate?
}
cv_mse[i] <- error/kfolds
}
min_lambda <- as.numeric(lambdas[which(cv_mse==min(cv_mse))])
beta <- opt.score(Y_train, X_train, R0, R1, min_lambda)
return(list(beta=beta, min_lambda=min_lambda, cv_mse=cv_mse))
}
library(R.matlab)
library(Matrix)
source('../opt.score_new1.R')
source('../cv_opt.score_new1.R')
source('../test and results.R')
eigV <- readMat('eigV.mat')$eigV
R0 <- readMM('../R0_642.mtx')
R1 <- readMM('../R1_642.mtx')
## sample generator
mysamples <- function(eigV, K=3, n=100){
r <- nrow(eigV)
c <- ncol(eigV)
X <- matrix(0, nrow=(K*n), ncol=r) # n samples by r nodes
for(k in 1:K){
for(i in 1:n){
# sample different bases v_1, v_2, v_3 for each individual
base <- eigV[, 1:3]
# sample 3 by 1 mean 0 coefficients of bases
coef <- matrix(c(rnorm(1,0,5), rnorm(1,0,3), rnorm(1,0,1)), nrow=3)
# add different mean vector to each class
mean_k <- 0.3*eigV[,(3+k)]
# the distribution of coefficients are same in the same class
x <- base%*%coef + rnorm(642, 0, 0.1) + mean_k
X[(k-1)*n+i,] <- x
}
}
y <- Reduce(rbind, sapply(1:K, function(x) rep(x,n)))
data <- cbind(y=y, X)
new_data <- data[sample(nrow(data)),]
y <- new_data[,1]
X <- new_data[,2:ncol(new_data)]
y_indice<-Reduce(rbind,lapply(y,function(x) {
y_i <- rep(0,K)
y_i[x]=1
return(y_i)
}))
df <- data.frame(X=X, y_indice=y_indice, y=y)
return(df)
}
## pre.smooth
pre.smooth <- function(ytrain, ytest, Xtrain, Xtest, R0, R1, lambdas){
n_train <- nrow(Xtrain)
n_test <- nrow(Xtest)
p <- ncol(Xtrain)
smooth.X_train <- matrix(0, nrow=n_train, ncol=p)
smooth.X_test <- matrix(0, nrow=n_test, ncol=p)
identity <- Diagonal(x=rep(1,p))
R0til.inv <- Matrix::Diagonal(x=1/Matrix::rowSums(R0))
S <- R1 %*% sqrt(R0til.inv)
max <- 0
select_lam <- 0
lda_model <- NULL
for(j in 1:length(lambdas)){
lambda <- lambdas[j]
Xstar <- rbind(identity, sqrt(lambda)*t(S))
for(i in 1:n_train){
x_obs <- c(Xtrain[i,], rep(0, p))
smooth.X_train[i,] <- as.vector(glmnet(Xstar, x_obs, lambda = 0, alpha=0,intercept = FALSE, standardize = FALSE, thresh = 1e-7)$beta)
}
df.train <- data.frame(cbind(smooth.X_train, y=ytrain))
fit <- lda(y~., df.train)
fitted.y <- predict(fit, df.train)$class
accuracy <- mean(ytrain == fitted.y)
if(accuracy>max) {
max <- accuracy
select_lam <- lambda
lda_model <- fit
}
}
Xstar <- rbind(identity, sqrt(select_lam)*t(S))
for(i in 1:n_test){
x_obs <- c(Xtest[i,], rep(0, p))
smooth.X_test[i,] <- as.vector(glmnet(Xstar, x_obs, lambda = 0, alpha=0,intercept = FALSE, standardize = FALSE, thresh = 1e-7)$beta)
}
df.test <- data.frame(cbind(smooth.X_test, y=ytest))
fitted.y <- predict(lda_model, df.test)$class
accuracy <- mean(ytest == fitted.y)
return(list(yhat = fitted.y, accuracy = accuracy))
}
samples <- mysamples(eigV,3,100)
samples <- samples[order(samples$y),]
test.index <- c(sample(1:n1, n1/3),sample(1:n2, n2/3),sample(1:n3, n3/3))
X <- as.matrix(samples[,1:642])
Y <- as.matrix(samples[,c("y_indice.1","y_indice.2","y_indice.3")])
y <- as.numeric(samples[,"y"])
X.train <- X[-test.index,]
Y.train <- Y[-test.index,]
y.train <- y[-test.index]
X.test <- X[test.index,]
Y.test <- Y[test.index,]
y.test <- y[test.index]
my.lambda.seq <- 10^seq(-5,5,1)
result1 <- rep(0,100)
for(i in 1:100){
fit.cv.opt <- cv_opt.score(Y.train, X.train, R0, R1, 5, my.lambda.seq)
model.sm.flda <- opt.score(Y.train, X.train, R0, R1, fit.cv.opt$min_lambda)
result.sm.flda <- test_acc(model.sm.flda, X.test, Y.test)
result1[i] <- result.sm.flda$accuracy
}
boxplot(result1)
result2 <- rep(0, 100)
for(i in 1:100){
result.presmooth <- pre.smooth(y.train, y.test, X.train, X.test, R0, R1, my.lambda.seq)
result2[i] <- result.presmooth$accuracy
}
# for(i in 1:100){
#         fit.cv.presmooth <- cv_pre.smooth(y.train, X.train, R0, R1, 5, my.lambda.seq)
#         result.presmooth <- pre.smooth(y.train, X.train, R0, R1, fit.cv.presmooth$min_lambda)
#         result2[i] <- result.presmooth$accuracy
# }
boxplot(result2)
eigV <- readMat('eigV.mat')$eigV
R0 <- readMM('../R0_642.mtx')
R1 <- readMM('../R1_642.mtx')
## sample generator
mysamples <- function(eigV, K=3, n=100){
r <- nrow(eigV)
c <- ncol(eigV)
X <- matrix(0, nrow=(K*n), ncol=r) # n samples by r nodes
for(k in 1:K){
for(i in 1:n){
# sample different bases v_1, v_2, v_3 for each individual
base <- eigV[, 1:3]
# sample 3 by 1 mean 0 coefficients of bases
coef <- matrix(c(rnorm(1,0,5), rnorm(1,0,3), rnorm(1,0,1)), nrow=3)
# add different mean vector to each class
mean_k <- 0.3*eigV[,(3+k)]
# the distribution of coefficients are same in the same class
x <- base%*%coef + rnorm(642, 0, 0.1) + mean_k
X[(k-1)*n+i,] <- x
}
}
y <- Reduce(rbind, sapply(1:K, function(x) rep(x,n)))
data <- cbind(y=y, X)
new_data <- data[sample(nrow(data)),]
y <- new_data[,1]
X <- new_data[,2:ncol(new_data)]
y_indice<-Reduce(rbind,lapply(y,function(x) {
y_i <- rep(0,K)
y_i[x]=1
return(y_i)
}))
df <- data.frame(X=X, y_indice=y_indice, y=y)
return(df)
}
## pre.smooth
pre.smooth <- function(ytrain, ytest, Xtrain, Xtest, R0, R1, lambdas){
n_train <- nrow(Xtrain)
n_test <- nrow(Xtest)
p <- ncol(Xtrain)
smooth.X_train <- matrix(0, nrow=n_train, ncol=p)
smooth.X_test <- matrix(0, nrow=n_test, ncol=p)
identity <- Diagonal(x=rep(1,p))
R0til.inv <- Matrix::Diagonal(x=1/Matrix::rowSums(R0))
S <- R1 %*% sqrt(R0til.inv)
max <- 0
select_lam <- 0
lda_model <- NULL
for(j in 1:length(lambdas)){
lambda <- lambdas[j]
Xstar <- rbind(identity, sqrt(lambda)*t(S))
for(i in 1:n_train){
x_obs <- c(Xtrain[i,], rep(0, p))
smooth.X_train[i,] <- as.vector(glmnet(Xstar, x_obs, lambda = 0, alpha=0,intercept = FALSE, standardize = FALSE, thresh = 1e-7)$beta)
}
df.train <- data.frame(cbind(smooth.X_train, y=ytrain))
fit <- lda(y~., df.train)
fitted.y <- predict(fit, df.train)$class
accuracy <- mean(ytrain == fitted.y)
if(accuracy>max) {
max <- accuracy
select_lam <- lambda
lda_model <- fit
}
}
Xstar <- rbind(identity, sqrt(select_lam)*t(S))
for(i in 1:n_test){
x_obs <- c(Xtest[i,], rep(0, p))
smooth.X_test[i,] <- as.vector(glmnet(Xstar, x_obs, lambda = 0, alpha=0,intercept = FALSE, standardize = FALSE, thresh = 1e-7)$beta)
}
df.test <- data.frame(cbind(smooth.X_test, y=ytest))
fitted.y <- predict(lda_model, df.test)$class
accuracy <- mean(ytest == fitted.y)
return(list(yhat = fitted.y, accuracy = accuracy))
}
my.lambda.seq <- 10^seq(-5,5,1)
result1 <- rep(0,100)
for(i in 1:100){
samples <- mysamples(eigV,3,100)
samples <- samples[order(samples$y),]
test.index <- c(sample(1:n1, n1/3),sample(1:n2, n2/3),sample(1:n3, n3/3))
X <- as.matrix(samples[,1:642])
Y <- as.matrix(samples[,c("y_indice.1","y_indice.2","y_indice.3")])
y <- as.numeric(samples[,"y"])
X.train <- X[-test.index,]
Y.train <- Y[-test.index,]
y.train <- y[-test.index]
X.test <- X[test.index,]
Y.test <- Y[test.index,]
y.test <- y[test.index]
fit.cv.opt <- cv_opt.score(Y.train, X.train, R0, R1, 5, my.lambda.seq)
model.sm.flda <- opt.score(Y.train, X.train, R0, R1, fit.cv.opt$min_lambda)
result.sm.flda <- test_acc(model.sm.flda, X.test, Y.test)
result1[i] <- result.sm.flda$accuracy
}
boxplot(result1)
result2 <- rep(0, 100)
for(i in 1:100){
samples <- mysamples(eigV,3,100)
samples <- samples[order(samples$y),]
test.index <- c(sample(1:n1, n1/3),sample(1:n2, n2/3),sample(1:n3, n3/3))
X <- as.matrix(samples[,1:642])
Y <- as.matrix(samples[,c("y_indice.1","y_indice.2","y_indice.3")])
y <- as.numeric(samples[,"y"])
X.train <- X[-test.index,]
Y.train <- Y[-test.index,]
y.train <- y[-test.index]
X.test <- X[test.index,]
Y.test <- Y[test.index,]
y.test <- y[test.index]
result.presmooth <- pre.smooth(y.train, y.test, X.train, X.test, R0, R1, my.lambda.seq)
result2[i] <- result.presmooth$accuracy
}
# for(i in 1:100){
#         fit.cv.presmooth <- cv_pre.smooth(y.train, X.train, R0, R1, 5, my.lambda.seq)
#         result.presmooth <- pre.smooth(y.train, X.train, R0, R1, fit.cv.presmooth$min_lambda)
#         result2[i] <- result.presmooth$accuracy
# }
boxplot(result2)
boxplot(result1)
str(result1)
sd(result1)
mean(result1)
samples <- mysamples(eigV,3,100)
samples <- samples[order(samples$y),]
X <- as.matrix(samples[,1:642])
Y <- as.matrix(samples[,c("y_indice.1","y_indice.2","y_indice.3")])
y <- as.numeric(samples[,"y"])
View(samples)
my.lambda.seq <- 10^seq(-5,5,1)
result1.100 <- rep(0,100)
for(i in 1:100){
samples <- mysamples(eigV,3,100)
samples <- samples[order(samples$y),]
X.train <- as.matrix(samples[,1:642])
Y.train <- as.matrix(samples[,c("y_indice.1","y_indice.2","y_indice.3")])
y.train <- as.numeric(samples[,"y"])
samples.test <- mysamples(eigV,3,100)
samples.test <- samples.test[order(samples.test$y),]
X.test <- as.matrix(samples.test[,1:642])
Y.test <- as.matrix(samples.test[,c("y_indice.1","y_indice.2","y_indice.3")])
y.test <- as.numeric(samples.test[,"y"])
fit.cv.opt <- cv_opt.score(Y.train, X.train, R0, R1, 5, my.lambda.seq)
model.sm.flda <- opt.score(Y.train, X.train, R0, R1, fit.cv.opt$min_lambda)
result.sm.flda <- test_acc(model.sm.flda, X.test, Y.test)
result1.100[i] <- result.sm.flda$accuracy
}
boxplot(result1.100)
result2.100 <- rep(0, 100)
for(i in 1:100){
samples <- mysamples(eigV,3,100)
samples <- samples[order(samples$y),]
X.train <- as.matrix(samples[,1:642])
Y.train <- as.matrix(samples[,c("y_indice.1","y_indice.2","y_indice.3")])
y.train <- as.numeric(samples[,"y"])
samples.test <- mysamples(eigV,3,100)
samples.test <- samples.test[order(samples.test$y),]
X.test <- as.matrix(samples.test[,1:642])
Y.test <- as.matrix(samples.test[,c("y_indice.1","y_indice.2","y_indice.3")])
y.test <- as.numeric(samples.test[,"y"])
result.presmooth <- pre.smooth(y.train, y.test, X.train, X.test, R0, R1, my.lambda.seq)
result2.100[i] <- result.presmooth$accuracy
}
boxplot(result2.100)
result1.50 <- rep(0,50)
for(i in 1:50){
samples <- mysamples(eigV,3,50)
samples <- samples[order(samples$y),]
X.train <- as.matrix(samples[,1:642])
Y.train <- as.matrix(samples[,c("y_indice.1","y_indice.2","y_indice.3")])
y.train <- as.numeric(samples[,"y"])
samples.test <- mysamples(eigV,3,50)
samples.test <- samples.test[order(samples.test$y),]
X.test <- as.matrix(samples.test[,1:642])
Y.test <- as.matrix(samples.test[,c("y_indice.1","y_indice.2","y_indice.3")])
y.test <- as.numeric(samples.test[,"y"])
fit.cv.opt <- cv_opt.score(Y.train, X.train, R0, R1, 5, my.lambda.seq)
model.sm.flda <- opt.score(Y.train, X.train, R0, R1, fit.cv.opt$min_lambda)
result.sm.flda <- test_acc(model.sm.flda, X.test, Y.test)
result1.50[i] <- result.sm.flda$accuracy
}
boxplot(result1.50)
result2.50 <- rep(0, 50)
for(i in 1:50){
samples <- mysamples(eigV,3,50)
samples <- samples[order(samples$y),]
X.train <- as.matrix(samples[,1:642])
Y.train <- as.matrix(samples[,c("y_indice.1","y_indice.2","y_indice.3")])
y.train <- as.numeric(samples[,"y"])
samples.test <- mysamples(eigV,3,50)
samples.test <- samples.test[order(samples.test$y),]
X.test <- as.matrix(samples.test[,1:642])
Y.test <- as.matrix(samples.test[,c("y_indice.1","y_indice.2","y_indice.3")])
y.test <- as.numeric(samples.test[,"y"])
result.presmooth <- pre.smooth(y.train, y.test, X.train, X.test, R0, R1, my.lambda.seq)
result2.50[i] <- result.presmooth$accuracy
}
boxplot(result2.50)
boxplot(result1.50)
boxplot(result2.100)
boxplot(result1.100)
mean(result1.50)
sd(result1.50)
mean(result1.100)
sd(result1.100)
mean(result2.50)
sd(result2.50)
mean(result2.100)
sd(result2.100)
c(result1.50, result1.100, result2.50, result2.100)
df<- df(accuracy=c(result1.50, result1.100, result2.50, result2.100), n=c(rep(150,50),rep(300,100),rep(150,50),rep(300,100)), method=c(rep("SM-FLDA",150), rep("Pre-smoothing",150)))
df<- data.frame(accuracy=c(result1.50, result1.100, result2.50, result2.100), n=c(rep(150,50),rep(300,100),rep(150,50),rep(300,100)), method=c(rep("SM-FLDA",150), rep("Pre-smoothing",150)))
str(df)
library(ggplot2)
df<- data.frame(accuracy=c(result1.50, result1.100, result2.50, result2.100), n=c(rep(150,50),rep(300,100),rep(150,50),rep(300,100)), method=c(rep("SM-FLDA",150), rep("Pre-smoothing",150)), method_n=c(rep("SM-FLDA, n=150", 50),rep("SM-FLDA, n=300",100),rep("Pre-smoothing, n=150", 50),rep("Pre-smoothing, n=300",100)))
ggplot(data=df, aes(method_n, accuracy))
ggplot(data=df, aes(method_n, accuracy)) +
geom_boxplot()
