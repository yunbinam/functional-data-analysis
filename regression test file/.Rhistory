v <- eigV[,sample(1:100,3)]
X <- simu_case1(n=100, v1=v[,1], v2=v[,2], v3=v[,3])
Y <- list(train=rbind(matrix(rep(c(1,0,0),30), ncol=3, byrow=TRUE),
matrix(rep(c(0,1,0),30), ncol=3, byrow=TRUE),
matrix(rep(c(0,0,1),30), ncol=3, byrow=TRUE)),
test=rbind(matrix(rep(c(1,0,0),40), ncol=3, byrow=TRUE),
matrix(rep(c(0,1,0),40), ncol=3, byrow=TRUE),
matrix(rep(c(0,0,1),40), ncol=3, byrow=TRUE)))
R0 <- Matrix::readMM('R0_642.mtx')
R1 <- Matrix::readMM('R1_642.mtx')
cv_opt.score(Y$train, X$train, R0, R1, 5, 10^(seq(-5,5,1)))
beta <- cv_opt.score(Y$train, X$train, R0, R1, 5, 10^(seq(-5,5,1)))$beta
X <- X$test
Y <- Y$test
K <- ncol(Y)
D <- 1/nrow(Y) * (t(Y)%*%Y) # K by K matrix
R0til.inv <- Matrix::Diagonal(x=1/Matrix::rowSums(R0))
L <- R1 %*% R0til.inv %*% R1
S <- Matrix::chol(L)
centered.X <- X - mean(X)
scoredX <- matrix(rep(0, nrow(Y)*(K-1)), ncol=K-1)
for(k in 1:(K-1)){
scoredX[,k] <- as.vector(centered.X %*% beta[,k])
}
mu_k <- matrix(0, nrow = K-1, ncol = K) # centroid vectors
pi_k <- vector(length = K) # priori probabilities
sigma <- matrix(0, nrow = K-1, ncol = K-1) # covariance matrix
for (l in 1:K){
tmp <- scoredX[Y[,l]==1,]
pi_k[l] <- nrow(tmp)/nrow(Y)
mu_k[,l] <- colMeans(tmp)
for(j in 1:nrow(tmp)){
sigma <- sigma + (tmp[j,] - mu_k[,l]) %*% t(tmp[j,] - mu_k[,l])
}
}
sigma <- sigma*(1/(nrow(Y)-K))
inv.sigma <- solve(sigma)
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhat <- apply(deltatrain, 1, which.max)
Y_label <- apply(Y, 1, which.max)
error <- mean((yhat-Y_label)^2)
error
Y_label
yhat
error <- mean(yhat!=Y_label)
error
cv_opt.score <- function(Y, X, R0, R1, kfolds, lambdas){
K <- ncol(Y)
nlambda <- length(lambdas)
cv_mse <- rep(0, nlambda)
fold <- sample(1:5, nrow(X), replace = TRUE)
for(i in 1:nlambda){
error=0
for(j in 1:kfolds){
X_train <- X[!fold==j,]
Y_train <- Y[!fold==j,]
X_valid <- X[fold==j,]
Y_valid <- Y[fold==j,]
beta <- opt.score(Y_train, X_train, R0, R1, lambdas[i])
D <- 1/nrow(Y_valid) * (t(Y_valid)%*%Y_valid) # K by K matrix
R0til.inv <- Matrix::Diagonal(x=1/Matrix::rowSums(R0))
L <- R1 %*% R0til.inv %*% R1
S <- Matrix::chol(L)
centered.X <- X_valid - mean(X_valid)
scoredX <- matrix(rep(0, nrow(Y_valid)*(K-1)), ncol=K-1)
for(k in 1:(K-1)){
scoredX[,k] <- as.vector(centered.X %*% beta[,k])
}
## LDA with two discriminant directions
mu_k <- matrix(0, nrow = K-1, ncol = K) # centroid vectors
pi_k <- vector(length = K) # priori probabilities
sigma <- matrix(0, nrow = K-1, ncol = K-1) # covariance matrix
for (l in 1:K){
tmp <- scoredX[Y_valid[,l]==1,]
pi_k[l] <- nrow(tmp)/nrow(Y_valid)
mu_k[,l] <- colMeans(tmp)
for(j in 1:nrow(tmp)){
sigma <- sigma + (tmp[j,] - mu_k[,l]) %*% t(tmp[j,] - mu_k[,l])
}
}
sigma <- sigma*(1/(nrow(Y_valid)-K))
inv.sigma <- solve(sigma)
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y_valid))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhattrain <- apply(deltatrain, 1, which.max)
Y_valid_label <- apply(Y_valid, 1, which.max)
error <- error + mean(yhattrain!=Y_valid_label) # training error rate?
}
cv_mse[i] <- error/kfolds
}
min_lambda <- as.numeric(lambdas[which(cv_mse==min(cv_mse))])
beta <- opt.score(Y_train, X_train, R0, R1, min_lambda)
return(list(beta=beta, min_lambda=min_lambda, cv_mse=cv_mse))
}
# cv_opt.score(Y$train, X$train, R0, R1, 5, 10^(seq(-5,5,1)))
beta <- cv_opt.score(Y$train, X$train, R0, R1, 5, 10^(seq(-5,5,1)))$beta
X <- X$test
Y <- Y$test
K <- ncol(Y)
D <- 1/nrow(Y) * (t(Y)%*%Y) # K by K matrix
R0til.inv <- Matrix::Diagonal(x=1/Matrix::rowSums(R0))
L <- R1 %*% R0til.inv %*% R1
S <- Matrix::chol(L)
centered.X <- X - mean(X)
scoredX <- matrix(rep(0, nrow(Y)*(K-1)), ncol=K-1)
for(k in 1:(K-1)){
scoredX[,k] <- as.vector(centered.X %*% beta[,k])
}
## LDA with two discriminant directions
mu_k <- matrix(0, nrow = K-1, ncol = K) # centroid vectors
pi_k <- vector(length = K) # priori probabilities
sigma <- matrix(0, nrow = K-1, ncol = K-1) # covariance matrix
for (l in 1:K){
tmp <- scoredX[Y[,l]==1,]
pi_k[l] <- nrow(tmp)/nrow(Y)
mu_k[,l] <- colMeans(tmp)
for(j in 1:nrow(tmp)){
sigma <- sigma + (tmp[j,] - mu_k[,l]) %*% t(tmp[j,] - mu_k[,l])
}
}
sigma <- sigma*(1/(nrow(Y)-K))
inv.sigma <- solve(sigma)
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhat <- apply(deltatrain, 1, which.max)
Y_label <- apply(Y, 1, which.max)
error <- mean(yhat!=Y_label)
error
beta <- cv_opt.score(Y$train, X$train, R0, R1, 5, 10^(seq(-5,5,1)))$beta
## simple example
eigV <- R.matlab::readMat('Simulation/eigV.mat')$eigV
v <- eigV[,sample(1:100,3)]
X <- simu_case1(n=100, v1=v[,1], v2=v[,2], v3=v[,3])
Y <- list(train=rbind(matrix(rep(c(1,0,0),30), ncol=3, byrow=TRUE),
matrix(rep(c(0,1,0),30), ncol=3, byrow=TRUE),
matrix(rep(c(0,0,1),30), ncol=3, byrow=TRUE)),
test=rbind(matrix(rep(c(1,0,0),40), ncol=3, byrow=TRUE),
matrix(rep(c(0,1,0),40), ncol=3, byrow=TRUE),
matrix(rep(c(0,0,1),40), ncol=3, byrow=TRUE)))
R0 <- Matrix::readMM('R0_642.mtx')
R1 <- Matrix::readMM('R1_642.mtx')
lambda <- 1
beta <- cv_opt.score(Y$train, X$train, R0, R1, 5, 10^(seq(-5,5,1)))$beta
## test
X <- X$test
Y <- Y$test
K <- ncol(Y)
D <- 1/nrow(Y) * (t(Y)%*%Y) # K by K matrix
R0til.inv <- Matrix::Diagonal(x=1/Matrix::rowSums(R0))
L <- R1 %*% R0til.inv %*% R1
S <- Matrix::chol(L)
centered.X <- X - mean(X)
scoredX <- matrix(rep(0, nrow(Y)*(K-1)), ncol=K-1)
## scoredX with using trained beta
for(k in 1:(K-1)){
scoredX[,k] <- as.vector(centered.X %*% beta[,k])
}
## LDA with two discriminant directions
mu_k <- matrix(0, nrow = K-1, ncol = K) # centroid vectors
pi_k <- vector(length = K) # priori probabilities
sigma <- matrix(0, nrow = K-1, ncol = K-1) # covariance matrix
for (l in 1:K){
tmp <- scoredX[Y[,l]==1,]
pi_k[l] <- nrow(tmp)/nrow(Y)
mu_k[,l] <- colMeans(tmp)
for(j in 1:nrow(tmp)){
sigma <- sigma + (tmp[j,] - mu_k[,l]) %*% t(tmp[j,] - mu_k[,l])
}
}
sigma <- sigma*(1/(nrow(Y)-K))
inv.sigma <- solve(sigma)
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhat <- apply(deltatrain, 1, which.max)
Y_label <- apply(Y, 1, which.max)
error <- mean(yhat!=Y_label)
error
yhat
Y_label
Y
getwd()
## simple example
eigV <- R.matlab::readMat('Simulation/eigV.mat')$eigV
rm(list=ls())
## simple example
eigV <- R.matlab::readMat('Simulation/eigV.mat')$eigV
v <- eigV[,sample(1:100,3)]
library(MASS)
simu_case1<-function(n=100,v1,v2,v3){
# v1,v2,v3 eigen functions of laplace-baltrami opetrator
# n is the number of samples in each class
Sigma<-diag(642)
class1<-mvrnorm(n = n, v1, Sigma)
class2<-mvrnorm(n = n, v2, Sigma)
class3<-mvrnorm(n = n, v3, Sigma)
X<-rbind(class1,class2,class3)
label<-c(rep(0,n),rep(1,n),rep(2,n))
indic1<-Reduce(rbind,lapply(1:n,function(x)matrix(c(1,0,0),ncol=3)))
indic2<-Reduce(rbind,lapply(1:n,function(x)matrix(c(0,1,0),ncol=3)))
indic3<-Reduce(rbind,lapply(1:n,function(x)matrix(c(0,0,1),ncol=3)))
indic<-rbind(indic1,indic2,indic3)
return(list(X=X,label=label,indic=indic))
}
data<-simu_case1(n=100,v1,v2,v3)
X <- simu_case1(n=100, v1=v[,1], v2=v[,2], v3=v[,3])
X <- simu_case1(n=100, v1=v[,1], v2=v[,2], v3=v[,3])
X$X
X$indic
sample <- simu_case1(n=100, v1=v[,1], v2=v[,2], v3=v[,3])
X <- sample$X
Y <- sample$indic
R0 <- Matrix::readMM('R0_642.mtx')
R1 <- Matrix::readMM('R1_642.mtx')
eigV <- R.matlab::readMat('eigV.mat')$eigV
v <- eigV[,sample(1:100,3)]
data<- simu_case1(n=100, v1=100*v[,1], v2=100*v[,2], v3=100*v[,3])
R0 <- Matrix::readMM('R0_642.mtx')
R1 <- Matrix::readMM('R1_642.mtx')
X<-data$X
indic<-data$indic
X_train<-X[c(1:30,101:130,201:230),]
Y_train<-indic[c(1:30,101:130,201:230),]
X_test<-X[-c(1:30,101:130,201:230),]
Y_test<-indic[-c(1:30,101:130,201:230),]
model<-opt.score(Y_train, X_train, R0, R1, 1)
K=3
beta=model$beta
mu_k <- model$mu_k # centroid vectors
pi_k <- model$pi_k # priori probabilities
inv.sigma <- model$inv.sigma # covariance matrix
scoredX<-cbind(X_test%*%beta[,1],X_test%*%beta[,2])
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y_test))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhat <- apply(deltatrain, 1, which.max)
Y_label <- apply(Y_test, 1, which.max)
accuracy<- mean(yhat==Y_label)
plot_data<-data.frame(scoredX,label=Y_label)
library(ggplot2)
ggplot(plot_data, aes(x=X1, y=X2,color=label)) +
geom_point(size=3)
opt.score <- function(Y, X, R0, R1, lambda){
Y <- as.matrix(Y)
X <- as.matrix(X)
n <- nrow(Y)
K <- ncol(Y)
s <- ncol(X)
## 1. prepare matrices
D <- 1/n * (t(Y)%*%Y) # K by K matrix
R0til.inv <- Matrix::Diagonal(x=1/Matrix::rowSums(R0))
L <- R1 %*% R0til.inv %*% R1
S <- Matrix::chol(L)
centered.X <- X - mean(X)
X.star <- rbind(centered.X, sqrt(lambda)*t(as.matrix(S)))
## 3. pre-specify theta1
Q <- as.matrix(rep(1, ncol(Y)))
beta <- matrix(rep(0, s*(K-1)), ncol=K-1)
scoredX <- matrix(rep(0, n*(K-1)), ncol=K-1)
## 4. loop
for(i in 1:(K-1)){
## a. compute theta
random.theta <- runif(K)
theta <- (diag(K) - as.matrix(Q[,i])%*%t(as.matrix(Q[,i]))%*%D) %*% random.theta
norm <- as.numeric(t(theta)%*%D%*%theta)
norm.theta <- theta * sqrt(1/norm)
## b. solve beta
mapped.Y <- Y %*% norm.theta
centered.mapped.Y <- mapped.Y - mean(mapped.Y)
Y.star <- c(centered.mapped.Y, rep(0, s))
beta[,i] <- as.numeric(glmnet::glmnet(X.star, Y.star,
lambda=0, alpha=0, intercept=FALSE,
standardize=FALSE, thresh=1e-7)$beta)
## scored X
scoredX[,i] <- as.vector(centered.X %*% beta[,i])
## c. add new theta
new.theta <- (diag(K) - as.matrix(Q[,i])%*%t(as.matrix(Q[,i]))%*%D) %*% solve(D)%*%t(Y)%*%X%*%beta[,i]
new.norm <- as.numeric(t(as.matrix(new.theta))%*%D%*%as.matrix(new.theta))
new.norm.theta <- new.theta * sqrt(1/new.norm)
Q <- cbind(Q, new.norm.theta)
## are the set of scores mutually orthogonal?
## pracma::dot(Q[,2],Q[,3])
}
return(beta)
}
eigV <- R.matlab::readMat('eigV.mat')$eigV
v <- eigV[,sample(1:100,3)]
data<- simu_case1(n=100, v1=100*v[,1], v2=100*v[,2], v3=100*v[,3])
R0 <- Matrix::readMM('R0_642.mtx')
R1 <- Matrix::readMM('R1_642.mtx')
X<-data$X
indic<-data$indic
X_train<-X[c(1:30,101:130,201:230),]
Y_train<-indic[c(1:30,101:130,201:230),]
X_test<-X[-c(1:30,101:130,201:230),]
Y_test<-indic[-c(1:30,101:130,201:230),]
model<-opt.score(Y_train, X_train, R0, R1, 1)
K=3
beta=model$beta
mu_k <- model$mu_k # centroid vectors
pi_k <- model$pi_k # priori probabilities
inv.sigma <- model$inv.sigma # covariance matrix
scoredX<-cbind(X_test%*%beta[,1],X_test%*%beta[,2])
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y_test))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhat <- apply(deltatrain, 1, which.max)
Y_label <- apply(Y_test, 1, which.max)
accuracy<- mean(yhat==Y_label)
plot_data<-data.frame(scoredX,label=Y_label)
library(ggplot2)
ggplot(plot_data, aes(x=X1, y=X2,color=label)) +
geom_point(size=3)
model
opt.score <- function(Y, X, R0, R1, lambda){
Y <- as.matrix(Y)
X <- as.matrix(X)
n <- nrow(Y)
K <- ncol(Y)
s <- ncol(X)
## 1. prepare matrices
D <- 1/n * (t(Y)%*%Y) # K by K matrix
R0til.inv <- Matrix::Diagonal(x=1/Matrix::rowSums(R0))
L <- R1 %*% R0til.inv %*% R1
S <- Matrix::chol(L)
centered.X <- X - mean(X)
X.star <- rbind(centered.X, sqrt(lambda)*t(as.matrix(S)))
## 3. pre-specify theta1
Q <- as.matrix(rep(1, ncol(Y)))
beta <- matrix(rep(0, s*(K-1)), ncol=K-1)
scoredX <- matrix(rep(0, n*(K-1)), ncol=K-1)
## 4. loop
for(i in 1:(K-1)){
## a. compute theta
random.theta <- runif(K)
theta <- (diag(K) - as.matrix(Q[,i])%*%t(as.matrix(Q[,i]))%*%D) %*% random.theta
norm <- as.numeric(t(theta)%*%D%*%theta)
norm.theta <- theta * sqrt(1/norm)
## b. solve beta
mapped.Y <- Y %*% norm.theta
#centered.mapped.Y <- mapped.Y - mean(mapped.Y)
Y.star <- c(mapped.Y, rep(0, s))
beta[,i] <- as.numeric(glmnet::glmnet(X.star, Y.star,
lambda=0, alpha=0, intercept=FALSE,
standardize=FALSE, thresh=1e-7)$beta)
## scored X
scoredX[,i] <- as.vector(centered.X %*% beta[,i])
## c. add new theta
new.theta <- (diag(K) - as.matrix(Q[,i])%*%t(as.matrix(Q[,i]))%*%D) %*% solve(D)%*%t(Y)%*%X%*%beta[,i]
new.norm <- as.numeric(t(as.matrix(new.theta))%*%D%*%as.matrix(new.theta))
new.norm.theta <- new.theta * sqrt(1/new.norm)
Q <- cbind(Q, new.norm.theta)
## are the set of scores mutually orthogonal?
## pracma::dot(Q[,2],Q[,3])
}
scoredX <- matrix(rep(0, nrow(Y)*(K-1)), ncol=K-1)
## scoredX with using trained beta
for(k in 1:(K-1)){
scoredX[,k] <- as.vector(centered.X %*% beta[,k])
}
## LDA with two discriminant directions
mu_k <- matrix(0, nrow = K-1, ncol = K) # centroid vectors
pi_k <- vector(length = K) # priori probabilities
sigma <- matrix(0, nrow = K-1, ncol = K-1) # covariance matrix
for (l in 1:K){
tmp <- scoredX[Y[,l]==1,]
pi_k[l] <- nrow(tmp)/nrow(Y)
mu_k[,l] <- colMeans(tmp)
for(j in 1:nrow(tmp)){
sigma <- sigma + (tmp[j,] - mu_k[,l]) %*% t(tmp[j,] - mu_k[,l])
}
}
sigma <- sigma*(1/(nrow(Y)-K))
inv.sigma <- solve(sigma)
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
return(model=list(beta=beta,inv.sigma =inv.sigma , mu_k= mu_k,pi_k=pi_k))
}
X<-data$X
indic<-data$indic
X_train<-X[c(1:30,101:130,201:230),]
Y_train<-indic[c(1:30,101:130,201:230),]
X_test<-X[-c(1:30,101:130,201:230),]
Y_test<-indic[-c(1:30,101:130,201:230),]
model<-opt.score(Y_train, X_train, R0, R1, 1)
K=3
beta=model$beta
mu_k <- model$mu_k # centroid vectors
pi_k <- model$pi_k # priori probabilities
inv.sigma <- model$inv.sigma # covariance matrix
scoredX<-cbind(X_test%*%beta[,1],X_test%*%beta[,2])
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y_test))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhat <- apply(deltatrain, 1, which.max)
Y_label <- apply(Y_test, 1, which.max)
accuracy<- mean(yhat==Y_label)
plot_data<-data.frame(scoredX,label=Y_label)
library(ggplot2)
ggplot(plot_data, aes(x=X1, y=X2,color=label)) +
geom_point(size=3)
ggplot(plot_data, aes(x=X1, y=X2, color=as.factor(label))) +
geom_point(size=3)
data<- simu_case1(n=100, v1=v[,1], v2=v[,2], v3=v[,3])
R0 <- Matrix::readMM('R0_642.mtx')
R1 <- Matrix::readMM('R1_642.mtx')
X<-data$X
indic<-data$indic
X_train<-X[c(1:30,101:130,201:230),]
Y_train<-indic[c(1:30,101:130,201:230),]
X_test<-X[-c(1:30,101:130,201:230),]
Y_test<-indic[-c(1:30,101:130,201:230),]
model<-opt.score(Y_train, X_train, R0, R1, 1)
K=3
beta=model$beta
mu_k <- model$mu_k # centroid vectors
pi_k <- model$pi_k # priori probabilities
inv.sigma <- model$inv.sigma # covariance matrix
scoredX<-cbind(X_test%*%beta[,1],X_test%*%beta[,2])
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y_test))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhat <- apply(deltatrain, 1, which.max)
Y_label <- apply(Y_test, 1, which.max)
accuracy<- mean(yhat==Y_label)
plot_data<-data.frame(scoredX,label=Y_label)
library(ggplot2)
ggplot(plot_data, aes(x=X1, y=X2, color=as.factor(label))) +
geom_point(size=3)
data<- simu_case1(n=100, v1=30*v[,1], v2=30*v[,2], v3=30*v[,3])
X<-data$X
indic<-data$indic
X_train<-X[c(1:30,101:130,201:230),]
Y_train<-indic[c(1:30,101:130,201:230),]
X_test<-X[-c(1:30,101:130,201:230),]
Y_test<-indic[-c(1:30,101:130,201:230),]
model<-opt.score(Y_train, X_train, R0, R1, 1)
K=3
beta=model$beta
mu_k <- model$mu_k # centroid vectors
pi_k <- model$pi_k # priori probabilities
inv.sigma <- model$inv.sigma # covariance matrix
scoredX<-cbind(X_test%*%beta[,1],X_test%*%beta[,2])
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y_test))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhat <- apply(deltatrain, 1, which.max)
Y_label <- apply(Y_test, 1, which.max)
accuracy<- mean(yhat==Y_label)
plot_data<-data.frame(scoredX,label=Y_label)
library(ggplot2)
ggplot(plot_data, aes(x=X1, y=X2, color=as.factor(label))) +
geom_point(size=3)
data<- simu_case1(n=100, v1=15*v[,1], v2=15*v[,2], v3=15*v[,3])
R0 <- Matrix::readMM('R0_642.mtx')
R1 <- Matrix::readMM('R1_642.mtx')
X<-data$X
indic<-data$indic
X_train<-X[c(1:30,101:130,201:230),]
Y_train<-indic[c(1:30,101:130,201:230),]
X_test<-X[-c(1:30,101:130,201:230),]
Y_test<-indic[-c(1:30,101:130,201:230),]
model<-opt.score(Y_train, X_train, R0, R1, 1)
K=3
beta=model$beta
mu_k <- model$mu_k # centroid vectors
pi_k <- model$pi_k # priori probabilities
inv.sigma <- model$inv.sigma # covariance matrix
scoredX<-cbind(X_test%*%beta[,1],X_test%*%beta[,2])
deltatrain <- matrix(0, ncol = K, nrow = nrow(Y_test))
for (t in 1:K){
deltatrain[,t] <- scoredX %*% inv.sigma %*% mu_k[,t]
deltatrain[,t] <- deltatrain[,t] - 0.5*as.numeric(t(mu_k[,t]) %*% inv.sigma %*% mu_k[,t]) + log(pi_k[t])
}
yhat <- apply(deltatrain, 1, which.max)
Y_label <- apply(Y_test, 1, which.max)
accuracy<- mean(yhat==Y_label)
plot_data<-data.frame(scoredX,label=Y_label)
library(ggplot2)
ggplot(plot_data, aes(x=X1, y=X2, color=as.factor(label))) +
geom_point(size=3)
sd(v[,1])
sd(v[,2])
sd(v[,3])
mean(v[,1])
mean(v[,2])
mean(v[,3])
