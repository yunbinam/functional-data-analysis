l.opt <- seq.l[which.min(mean.cv.mse)]
return(mean.cv.mse)
}
simulation(data, R0, R1)
y <- sample$A[,1]
X <- sample$A[,2:ncol(sample$A)]
y=data$A[,1]
X=data$A[,2:ncol(data$A)]
n <- length(y)
folds <- cut(1:n, breaks=5, labels=FALSE)
folds
folds <- sample(folds)
folds
seq.l = seq(10^(seq(-5,5,1)))
mse <- matrix(0, ncol=length(seq.l), nrow=K)
mse <- matrix(0, ncol=length(seq.l), nrow=5)
mse
for(i in 1:K){
X.train <- X[folds!=i,]
y.train <- y[folds!=i]
X.test <- X[folds==i,]
y.test <- y[folds==i]
mse[i,] <- sapply(seq.l,
FUN = function(l.val){
fit.y <- spareg(y.train, X.train, R0, R1, l.val)
predict <- fit.y$intercept + X.test%*%fit.y$beta
mean((predict-y.test)^2)
}
)
}
for(i in 1:K){
X.train <- X[folds!=i,]
y.train <- y[folds!=i]
X.test <- X[folds==i,]
y.test <- y[folds==i]
mse[i,] <- sapply(seq.l,
FUN = function(l.val){
fit.y <- spareg(y.train, X.train, R0, R1, l.val)
predict <- fit.y$intercept + X.test%*%fit.y$beta
mean((predict-y.test)^2)
}
)
}
K=5
for(i in 1:K){
X.train <- X[folds!=i,]
y.train <- y[folds!=i]
X.test <- X[folds==i,]
y.test <- y[folds==i]
mse[i,] <- sapply(seq.l,
FUN = function(l.val){
fit.y <- spareg(y.train, X.train, R0, R1, l.val)
predict <- fit.y$intercept + X.test%*%fit.y$beta
mean((predict-y.test)^2)
}
)
}
mse
mean.cv.mse <- colMeans(mse)
mean.cv.mse
l.opt <- seq.l[which.min(mean.cv.mse)]
l.opt
seq.l
seq.l = seq(10^(seq(-5,5,1)))
seq.l
10^(seq(-5,5,1))
mse[i,] <- sapply(seq.l,
FUN = function(l.val){
fit.y <- spareg(y.train, X.train, R0, R1, l.val)
predict <- fit.y$intercept + X.test%*%fit.y$beta
mean((predict-y.test)^2)
}
)
mse
mean.cv.mse <- colMeans(mse)
l.opt <- seq.l[which.min(mean.cv.mse)]
l.opt
seq.l = 10^(seq(-5,5,1)
)
mse[i,] <- sapply(seq.l,
FUN = function(l.val){
fit.y <- spareg(y.train, X.train, R0, R1, l.val)
predict <- fit.y$intercept + X.test%*%fit.y$beta
mean((predict-y.test)^2)
}
)
mse
mean.cv.mse <- colMeans(mse)
l.opt <- seq.l[which.min(mean.cv.mse)]
l.opt
simulation <- function(sample, R0, R1,
p = 0.5, K = 5, seq.l = 10^(seq(-5,5,1)),
random.folds = FALSE){
y <- sample$A[,1]
X <- sample$A[,2:ncol(sample$A)]
n <- length(y)
train <- sample(1:n, p * n)
test <- -train
X_train <- X[train,]
X_test <- X[test,]
y_train <- y[train]
y_test <- y[test]
n_train <- length(y_train)
folds <- cut(1:y_train, breaks=K, labels=FALSE)
if(random.folds){
folds <- sample(folds)
}
mse <- matrix(0, ncol=length(seq.l), nrow=K)
for(i in 1:K){
X.train <- X_train[folds!=i,]
y.train <- y_train[folds!=i]
X.test <- X_train[folds==i,]
y.test <- y_train[folds==i]
mse[i,] <- sapply(seq.l,
FUN = function(l.val){
fit.y <- spareg(y.train, X.train, R0, R1, l.val)
predict <- fit.y$intercept + X.test%*%fit.y$beta
mean((predict-y.test)^2)
}
)
}
mean.cv.mse <- colMeans(mse)
l.opt <- seq.l[which.min(mean.cv.mse)]
fit.y <- spareg(y_train, X_train, R0, R1, l.opt)
predict <- X_test%*%fit.y$beta+fit.y$intercept
mse <- mean((y_test-predict)^2)
return(mse)
}
simulation(data, R0,R1)
sample=data
y <- sample$A[,1]
X <- sample$A[,2:ncol(sample$A)]
n <- length(y)
train <- sample(1:n, p * n)
test <- -train
X_train <- X[train,]
X_test <- X[test,]
y_train <- y[train]
y_test <- y[test]
n_train <- length(y_train)
p=0.5
K=5
seq.l = 10^(seq(-5,5,1))
y <- sample$A[,1]
X <- sample$A[,2:ncol(sample$A)]
n <- length(y)
train <- sample(1:n, p * n)
test <- -train
X_train <- X[train,]
X_test <- X[test,]
y_train <- y[train]
y_test <- y[test]
n_train <- length(y_train)
folds <- cut(1:y_train, breaks=K, labels=FALSE)
if(random.folds){
folds <- sample(folds)
}
folds <- cut(1:y_train, breaks=K, labels=FALSE)
folds <- cut(1:n_train, breaks=K, labels=FALSE)
folds
simulation <- function(sample, R0, R1,
p = 0.5, K = 5, seq.l = 10^(seq(-5,5,1)),
random.folds = FALSE){
y <- sample$A[,1]
X <- sample$A[,2:ncol(sample$A)]
n <- length(y)
train <- sample(1:n, p * n)
test <- -train
X_train <- X[train,]
X_test <- X[test,]
y_train <- y[train]
y_test <- y[test]
n_train <- length(y_train)
folds <- cut(1:n_train, breaks=K, labels=FALSE)
if(random.folds){
folds <- sample(folds)
}
mse <- matrix(0, ncol=length(seq.l), nrow=K)
for(i in 1:K){
X.train <- X_train[folds!=i,]
y.train <- y_train[folds!=i]
X.test <- X_train[folds==i,]
y.test <- y_train[folds==i]
mse[i,] <- sapply(seq.l,
FUN = function(l.val){
fit.y <- spareg(y.train, X.train, R0, R1, l.val)
predict <- fit.y$intercept + X.test%*%fit.y$beta
mean((predict-y.test)^2)
}
)
}
mean.cv.mse <- colMeans(mse)
l.opt <- seq.l[which.min(mean.cv.mse)]
fit.y <- spareg(y_train, X_train, R0, R1, l.opt)
predict <- X_test%*%fit.y$beta+fit.y$intercept
mse <- mean((y_test-predict)^2)
return(mse)
}
simulation(sample, R0,R1)
replicate(10, simulation(sample, R0,R1))
mse <- replicate(100, simulation(sample, R0, R1, seq.l = 10^(seq(-3,3,1))))
boxplot(mse)
eigV=readMat('Simulation/eigV.mat')
str(eigV)
eigV=readMat('Simulation/eigV.mat')$eigV
head(eigV)
str(eigV)
apply(eigV, 2, sd)
sd(eigV[,1])
sd(eigV[,2])
sd(eigV[,10])
apply(eigV, 2, mean)
sample(1)
sample(1,100)
?Sample
?sample
sample(1:100,1)
sample(1:100,1)
sample(1:100,1)
sample(1:100,1)
m=10
base_index <- sample(1:ncol(eigV),m)
base_index
base <- eigV[, base_index]
base
str(base)
base_index <- sample(1:ncol(eigV),m)
coef <- rnorm(m, 0, v)
v=9
coef <- rnorm(m, 0, v)
x <- coef%*%base
str(coef)
str(base)
coef <- t(rnorm(m, 0, v))
str(coef)
coef
coef <- matrix(rnorm(m, 0, v), nrow=m)
str(coef)
coef
x <- coef%*%base
str(base)
str(coef)
coef*base
t(coef)*base
coef <- rnorm(m, 0, v)
coef*base
str(coef*base)
class(coef)
class(base)
randomLinCom
library(alr3)
install.packages("alr3")
library(alr3)
library(remotes)
std <- function(x){
s <- sd(x)
if( s > 0) (x-mean(x))/s else x}
as.vector(apply(X, 2, std)%*% as.vector(2*rnorm(dim(X)[2])-1) )
str(X)
str(as.vector(2*rnorm(dim(X)[2])-1))
str(as.vector(base))
base <- eigV[, base_index]
coef <- matrix(rnorm(m, 0, v), nrow=m)
str(base)
str(coef)
x <- base%*%coef
str(x)
rnorm(642, 0,1)
(rnorm(642, 0,1))
x
head(x)
head(x+1)
head(x+seq(1,642))
str(eigV)
mysamples <- function(eigV, m, n, v, noise_x, noise_y){
r <- nrow(eigV)
c <- ncol(eigV)
# pick an eigenfunction as beta
beta <- matrix(eigV[, sample(1:c,1)], ncol=1) # r by 1 vector
X <- matrix(0, nrow=r, ncol=n) # r (nodes) by n samples
for(i in 1:n){
base_index <- sample(1:col,m)
base <- eigV[, base_index]
coef <- matrix(rnorm(m, 0, v), nrow=m)
x <- base%*%coef + rnorm(r, 0, noise_x)
X[,i] <- x
}
y <- t(X) %*% beta + rnorm(n, 0, noise_y) # n by 1 vector
A <- cbind(y, t(X))
return(list(A=A, y=y, X=X))
}
mysamples(eigV, 10, 200, 4, 4, 4)
# Arguments:
# eigV: r by c matrix, c eigenfunctions of the Laplace-Beltrami operator
# m: the number of base eigenfunctions using in linear combination to generate x_ii
# n: the number of generating samples
# v: variance of coefficients in linear combination
# noise_x: varaince of random noise on x
# noise_y: varaince of random noise on y
mysamples <- function(eigV, m, n, v, noise_x, noise_y){
r <- nrow(eigV)
c <- ncol(eigV)
# pick an eigenfunction as beta
beta <- matrix(eigV[, sample(1:c,1)], ncol=1) # r by 1 vector
X <- matrix(0, nrow=r, ncol=n) # r (nodes) by n samples
for(i in 1:n){
base_index <- sample(1:c,m)
base <- eigV[, base_index]
coef <- matrix(rnorm(m, 0, v), nrow=m)
x <- base%*%coef + rnorm(r, 0, noise_x)
X[,i] <- x
}
y <- t(X) %*% beta + rnorm(n, 0, noise_y) # n by 1 vector
A <- cbind(y, t(X))
return(list(A=A, y=y, X=X))
}
mysamples(eigV, 10, 200, 4,4,4)
str(mysamples(eigV, 10, 200, 4,4,4))
mysamples <- function(eigV, m, n, v, noise_x, noise_y){
r <- nrow(eigV)
c <- ncol(eigV)
# pick an eigenfunction as beta
beta <- matrix(eigV[, sample(1:c,1)], ncol=1) # r by 1 vector
X <- matrix(0, nrow=n, ncol=r) # n samples by r nodes
for(i in 1:n){
base_index <- sample(1:c,m)
base <- eigV[, base_index]
coef <- matrix(rnorm(m, 0, v), nrow=m)
x <- base%*%coef + rnorm(r, 0, noise_x)
X[i,] <- x
}
y <- X %*% beta + rnorm(n, 0, noise_y) # n by 1 vector
A <- cbind(y, X)
return(list(A=A, y=y, X=X))
}
str(mysamples(eigV, 10, 200, 4,4,4))
source('Simulation/sample generator.R')
source('spareg.R')
source('CV_spareg.R')
source('cv_smooth_x.R')
source('smooth_x_reg.R')
source('Simulation/sample generator.R')
# reading the data
eigV <- readMat('Simulation/eigV.mat')$eigV
R0 <- readMM('R0_642.mtx')
R1 <- readMM('R1_642.mtx')
sample <- mysamples(eigV, 15, 500, 4, 1, 9)
mse <- replicate(100, simulation(sample, R0, R1, seq.l = 10^(seq(-3,3,1))))
boxplot(mse)
getwd()
source('spareg.R')
source('CV_spareg.R')
source('cv_smooth_x.R')
source('smooth_x_reg.R')
source('Simulation/sample generator.R')
eigV <- readMat('Simulation/eigV.mat')$eigV
R0 <- readMM('R0_642.mtx')
R1 <- readMM('R1_642.mtx')
sample=mysamples(eigV, 15, 500, 4, 1, 9)
simulation <- function(sample, R0, R1,
p = 0.5, K = 5, seq.l = 10^(seq(-5,5,1)),
random.folds = FALSE){
y <- sample$y
X <- sample$X
n <- length(y)
train <- sample(1:n, p * n)
test <- -train
X_train <- X[train,]
X_test <- X[test,]
y_train <- y[train]
y_test <- y[test]
n_train <- length(y_train)
folds <- cut(1:n_train, breaks=K, labels=FALSE)
if(random.folds){
folds <- sample(folds)
}
mse <- matrix(0, ncol=length(seq.l), nrow=K)
for(i in 1:K){
X.train <- X_train[folds!=i,]
y.train <- y_train[folds!=i]
X.test <- X_train[folds==i,]
y.test <- y_train[folds==i]
mse[i,] <- sapply(seq.l,
FUN = function(l.val){
fit.y <- spareg(y.train, X.train, R0, R1, l.val)
predict <- fit.y$intercept + X.test%*%fit.y$beta
mean((predict-y.test)^2)
}
)
}
mean.cv.mse <- colMeans(mse)
l.opt <- seq.l[which.min(mean.cv.mse)]
fit.y <- spareg(y_train, X_train, R0, R1, l.opt)
predict <- X_test%*%fit.y$beta+fit.y$intercept
mse <- mean((y_test-predict)^2)
return(mse)
}
oneSim <- function(sample, R0, R1,
p = 0.5, K = 5, seq.l = 10^(seq(-5,5,1))){
y <- sample$y
X <- sample$X
n <- length(y)
train <- sample(1:n, p * n)
test <- -train
X_train <- X[train,]
X_test <- X[test,]
y_train <- y[train]
y_test <- y[test]
cv.fit <- cv_spareg(y_train,X_train,R0,R1,K,seq.l)
l.opt <- cv.fit$min_lambda
coef <- spareg(y_train, X_train, R0, R1, l.opt)
predict <- X_test%*%coef$beta+coef$intercept
mse <- mean((y_test-predict)^2)
return(mse)
}
library(ggplot2)
rep("1", 3)
rep("mse", 3)
mse
length(mse)
data.frame("ma"=c(1,1,1), "fa"=c(2,2,2))
mse1 <- replicate(100, simulation(sample, R0, R1, random.folds=TRUE))
mse2 <- replicate(100, simulation(sample, R0, R1, random.folds=FALSE))
mse3 <- replicate(100, oneSim(sample, R0, R1))
rst <- data.frame("f"=c(rep("f1", length(mse1)), rep("f2", length(mse2)), rep("f3", length(mse3))),
"MSE"=c(mse1, mse2, mse3))
ggplot(rst, aes(x=f, y=MSE)) +
geom_boxplot()
boxplot(mse3)
ggplot(rst,aes(x=f,y=MSE))+geom_boxplot()
mse3
boxplot(mse3)
boxplot(mse3)
ggplot(rst, aes(x=f, y=MSE)) +
geom_boxplot()
ggplot(rst, aes(x=f, y=MSE, fill=f)) +
geom_boxplot()
oneSim <- function(sample, R0, R1, method = "cv_spareg",
p = 0.5, K = 5, seq.l = 10^(seq(-5,5,1))){
y <- sample$y
X <- sample$X
n <- length(y)
train <- sample(1:n, p * n)
test <- -train
X_train <- X[train,]
X_test <- X[test,]
y_train <- y[train]
y_test <- y[test]
if(method=="cv_spareg"){
spareg.fit <- cv_spareg(y_train,X_train,R0,R1,K,seq.l)
l.opt <- spareg.fit$min_lambda
coef <- spareg(y_train, X_train, R0, R1, l.opt)
}
if(method=="smooth_recon_x"){
recon.fit <- smooth_recon_X(y_train,X_train,R0,R1,seq.l)
coef <- recon.fit$coef
}
if(method=="cv_smooth_ols_x"){
ols.fit <- cv_smooth_ols_x(y_train,X_train,R0,R1,seq.l)
coef <- ols.fit$coef
}
predict <- X_test%*%coef$beta+coef$intercept
mse <- mean((y_test-predict)^2)
return(mse)
}
# Arguments:
# sample:       generated samples, matrix A: [y X]
# R0:           mass matrix
# R1:           stiffness matrix
# method:       one of comparing methods: cv_spareg, smooth_recon_x, cv_smooth_ols_x
# p:            proportion of training samples
# K:            the number of cross validation folds
# seq.l:        sequence of tuning parameter lambda
# random.folds: logical
oneSim <- function(sample, R0, R1, method = "cv_spareg",
p = 0.5, K = 5, seq.l = 10^(seq(-5,5,1))){
y <- sample$y
X <- sample$X
n <- length(y)
train <- sample(1:n, p * n)
test <- -train
X_train <- X[train,]
X_test <- X[test,]
y_train <- y[train]
y_test <- y[test]
if(method=="cv_spareg"){
spareg.fit <- cv_spareg(y_train,X_train,R0,R1,K,seq.l)
coef <- spareg$coef
}
if(method=="smooth_recon_x"){
recon.fit <- smooth_recon_X(y_train,X_train,R0,R1,seq.l)
coef <- recon.fit$coef
}
if(method=="cv_smooth_ols_x"){
ols.fit <- cv_smooth_ols_x(y_train,X_train,R0,R1,K,seq.l)
coef <- ols.fit$coef
}
predict <- X_test%*%coef$beta+coef$intercept
mse <- mean((y_test-predict)^2)
return(mse)
}
?mysamples
mysamples
sample_noise <- mysamples(eigV, 10, 500, 4, 4, 1)
sample_smooth <- mysamples(eigV, 10, 500, 4, 0, 1)
library(dplyr)
rst <- data.frame("method"=c(rep("Smoothing beta", length(mse1)),
rep("Smoothing x with recon", length(mse2)),
rep("Smoothing x with cv", length(mse3))),
"sample"=c(rep("Noisy", length(mse1)+length(mse2)+length(mse3)),
rep("Smooth", length(mse1_s)+length(mse2_s)+length(mse3_s))),
"MSE"=c(mse1, mse2, mse3))
